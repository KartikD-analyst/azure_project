
ğŸ“˜ README.md
ğŸš€ AdventureWorks Data Engineering Project

This repository contains a complete data engineering workflow built on the AdventureWorks dataset. It demonstrates skills in data ingestion, transformation, SQL development, and Databricks notebooksâ€”with a focus on Azure Data Engineering practices.

ğŸ“‚ Repository Structure
â”œâ”€â”€ AdventureWorks_Calendar.csv        # Calendar dimension data  
â”œâ”€â”€ AdventureWorks_Customer.csv        # Customer dimension data  
â”œâ”€â”€ AdventureWorks_Product.csv         # Product master data  
â”œâ”€â”€ AdventureWorks_ProductCategory.csv # Product categories  
â”œâ”€â”€ AdventureWorks_ProductSubcat.csv   # Product subcategories  
â”œâ”€â”€ AdventureWorks_Returns.csv         # Product returns data  
â”œâ”€â”€ AdventureWorks_Sales_Order.csv     # Sales orders  
â”œâ”€â”€ AdventureWorks_Sales_OrderDetail.csv # Sales order details  
â”œâ”€â”€ AdventureWorks_Sales_Reason.csv    # Sales reason mapping  
â”œâ”€â”€ AdventureWorks_Territory.csv       # Territory mapping  
â”œâ”€â”€ Create Views Gold.sql              # SQL script for Gold layer views  
â”œâ”€â”€ silver_layer.dbc                   # Databricks notebook (DBC format)  
â”œâ”€â”€ silver_layer_refer.ipynb           # Databricks/Spark notebook (Python)  
â”œâ”€â”€ git.json / git (1).json            # Config files  
â””â”€â”€ README.md                          # Project documentation  

ğŸ› ï¸ Key Features

Data Ingestion: AdventureWorks data loaded into storage for processing.

Silver Layer: Data transformations using PySpark/Databricks notebooks.

Gold Layer: Final business-ready tables created via SQL scripts (Create Views Gold.sql).

Data Modeling: Dimensional model including fact & dimension tables (Calendar, Customer, Product, Sales, etc.).

Notebooks: Code-based transformations & exploratory analysis (silver_layer.dbc, silver_layer_refer.ipynb).

ğŸ§° Tools & Technologies

Azure Databricks â€“ Data processing and transformation

Azure Data Lake / Storage â€“ Data storage

SQL â€“ Gold layer transformations & reporting

PySpark â€“ ETL pipeline development

AdventureWorks Dataset â€“ Sample retail dataset

ğŸ“Š Workflow

Raw Layer â€“ Import CSV files (AdventureWorks datasets).

Silver Layer â€“ Clean & transform using Databricks notebooks.

Gold Layer â€“ Create business views using SQL (Create Views Gold.sql).

Consumption â€“ Data ready for reporting & dashboards (Power BI / Synapse).

ğŸš¦ How to Use

Clone the repository

git clone https://github.com/yourusername/AdventureWorks-DataEngineering.git


Import .csv files into Azure Data Lake.

Upload .dbc / .ipynb notebooks into Databricks workspace.

Run silver layer transformations.

Execute Create Views Gold.sql for gold layer tables.

ğŸ“Œ Future Enhancements

Add Bronze Layer raw ingestion pipeline.

Automate ETL with Azure Data Factory.

Integrate with Power BI dashboards.
